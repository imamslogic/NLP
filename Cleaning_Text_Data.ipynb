{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Cleaning Text Data.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imamslogic/NLP/blob/main/Cleaning_Text_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu__ZdNH2RJr"
      },
      "source": [
        "# Text Cleaning in Python Using NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m-a1mSq2RJ2"
      },
      "source": [
        "#warnings :)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTJhfER22RJ4"
      },
      "source": [
        "#Creating bunch of sentences\n",
        "raw_docs = [\"I am writing some very basic english sentences\",\n",
        "\"I'm just writing it for the demo PURPOSE to make audience understand the basics .\",\n",
        "\"The point is to _learn HOW it works_ on #simple # data.\"]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSuH6-nC2RJ4"
      },
      "source": [
        "#importing nltk package\n",
        "import nltk"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKJger-u2RJ5"
      },
      "source": [
        "#nltk.download()\n",
        "\n",
        "#python -m nltk.downloader all"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVloxF_y2RJ6"
      },
      "source": [
        "# Step 1 - convert to lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx0F3xLV2RJ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f24cf50-7a90-486b-a8dc-c4f1ae5ea5a3"
      },
      "source": [
        "import string\n",
        "raw_docs = [doc.lower() for doc in raw_docs]\n",
        "print(raw_docs)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i am writing some very basic english sentences', \"i'm just writing it for the demo purpose to make audience understand the basics .\", 'the point is to _learn how it works_ on #simple # data.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L83mf4pS24kP",
        "outputId": "b87ad84e-c8a5-4a2e-caf4-08a108d9fc04"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ3lT1E92RJ9"
      },
      "source": [
        "# Step 2 - Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iANAB-Sk2RJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7455b86c-0b17-4318-b80f-817d19922d61"
      },
      "source": [
        "# word tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n",
        "print(tokenized_docs)\n",
        "\n",
        "print(\"#######################################################################################\")\n",
        "\n",
        "#Sentence tokenization\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sent_token = [sent_tokenize(doc) for doc in raw_docs]\n",
        "print(sent_token)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['i', 'am', 'writing', 'some', 'very', 'basic', 'english', 'sentences'], ['i', \"'m\", 'just', 'writing', 'it', 'for', 'the', 'demo', 'purpose', 'to', 'make', 'audience', 'understand', 'the', 'basics', '.'], ['the', 'point', 'is', 'to', '_learn', 'how', 'it', 'works_', 'on', '#', 'simple', '#', 'data', '.']]\n",
            "#######################################################################################\n",
            "[['i am writing some very basic english sentences'], [\"i'm just writing it for the demo purpose to make audience understand the basics .\"], ['the point is to _learn how it works_ on #simple # data.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOYKDBlz2RJ-"
      },
      "source": [
        "# Step 3 - Punctuation Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsQPnS4M2RJ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec461925-87a4-4c78-9d89-45cd5c36dc6c"
      },
      "source": [
        "# Removing punctuation\n",
        "import re\n",
        "regex = re.compile('[%s]' % re.escape(string.punctuation)) #see documentation here: http://docs.python.org/2/library/string.html\n",
        "\n",
        "tokenized_docs_no_punctuation = []\n",
        "\n",
        "for review in tokenized_docs:\n",
        "    new_review = []\n",
        "    for token in review:\n",
        "        new_token = regex.sub(u'', token)\n",
        "        if not new_token == u'':\n",
        "            new_review.append(new_token)\n",
        "    \n",
        "    tokenized_docs_no_punctuation.append(new_review)\n",
        "    \n",
        "print(tokenized_docs_no_punctuation)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['i', 'am', 'writing', 'some', 'very', 'basic', 'english', 'sentences'], ['i', 'm', 'just', 'writing', 'it', 'for', 'the', 'demo', 'purpose', 'to', 'make', 'audience', 'understand', 'the', 'basics'], ['the', 'point', 'is', 'to', 'learn', 'how', 'it', 'works', 'on', 'simple', 'data']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMcs2kIQ3CRg",
        "outputId": "2256ef2f-62cb-429c-8553-1e6f8fe54cff"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReEXVeCy2RJ_"
      },
      "source": [
        "# Step 4 - Removing Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYFSk5_x2RKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8842fb1-4d84-42e0-ed5d-0bc6c1ccfea0"
      },
      "source": [
        "# Cleaning text of stopwords\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "tokenized_docs_no_stopwords = []\n",
        "\n",
        "for doc in tokenized_docs_no_punctuation:\n",
        "    new_term_vector = []\n",
        "    for word in doc:\n",
        "        if not word in stopwords.words('english'):\n",
        "            new_term_vector.append(word)\n",
        "    \n",
        "    tokenized_docs_no_stopwords.append(new_term_vector)\n",
        "\n",
        "print(tokenized_docs_no_stopwords)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['writing', 'basic', 'english', 'sentences'], ['writing', 'demo', 'purpose', 'make', 'audience', 'understand', 'basics'], ['point', 'learn', 'works', 'simple', 'data']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN9RPJeE3J2m",
        "outputId": "9951d805-f212-4a53-9225-ec8d066a99fc"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjslRpAH2RKB"
      },
      "source": [
        "# Step 5- Stemming and Lemmantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkEmp-W42RKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed4e8e11-8582-44f1-f2a7-45c19b41277e"
      },
      "source": [
        "# Stemming and Lemmatization\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "wordnet = WordNetLemmatizer()\n",
        "\n",
        "preprocessed_docs = []\n",
        "\n",
        "for doc in tokenized_docs_no_stopwords:\n",
        "    final_doc = []\n",
        "    for word in doc:\n",
        "        #final_doc.append(porter.stem(word))\n",
        "        final_doc.append(wordnet.lemmatize(word))\n",
        "    \n",
        "    preprocessed_docs.append(final_doc)\n",
        "\n",
        "print(preprocessed_docs)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['writing', 'basic', 'english', 'sentence'], ['writing', 'demo', 'purpose', 'make', 'audience', 'understand', 'basic'], ['point', 'learn', 'work', 'simple', 'data']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF65U_ia3RaM",
        "outputId": "382766cd-c2ba-4e85-b15e-d7a2c8634643"
      },
      "source": [
        "pip install normalise"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: normalise in /usr/local/lib/python3.6/dist-packages (0.1.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from normalise) (1.19.4)\n",
            "Requirement already satisfied: roman in /usr/local/lib/python3.6/dist-packages (from normalise) (3.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from normalise) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from normalise) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from normalise) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->normalise) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->normalise) (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhGwtqne3cob",
        "outputId": "23234153-45f0-4f4a-fa39-a7a896d335b4"
      },
      "source": [
        "nltk.download('brown')\r\n",
        "nltk.download('names')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "nltk.download('universal_tagset')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVZiaKz62RKB"
      },
      "source": [
        "# Advance cleaning technique 1 - Normalization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sDWCxt32RKC"
      },
      "source": [
        "text = \"On the 30th Jan 2020,Corona virus hit India with 1st case in kerala  anywhere, G.O.I started acting and allocated fund of 17287 Crores I.N.R\""
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRgGTCl82RKC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8c90a6dd-3c8c-408c-97c0-f80398c01b71"
      },
      "source": [
        "from normalise import normalise\n",
        "\n",
        "custom_abbr = {\n",
        "    \"G.O.I\": \"Government Of India\",\n",
        "    \"I.N.R\": \"Indian Rupees\",\n",
        "    \"ttyl\":\"talk to you later\"\n",
        "    \n",
        "}\n",
        "\n",
        "normalized_tokens = normalise(word_tokenize(text), user_abbrevs=custom_abbr, verbose=False)\n",
        "display(f\"Normalized text: {' '.join(normalized_tokens)}\")"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Normalized text: On the thirtieth of Jan twenty twenty , Corona virus hit India with first case in kerala anywhere , Government Of India started acting and allocated fund of seventeen thousand, two hundred and eighty seven Crores Indian Rupees'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdBZOOYa2RKD"
      },
      "source": [
        "# Advance cleaning technique 2 - Type corection Using pyspellchecker, autocorrect and textblob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dei5QUzzF21a",
        "outputId": "c6e70654-a3f5-4386-e035-6281aa06e0a7"
      },
      "source": [
        "pip install pyspellchecker"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.6/dist-packages (0.5.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5gyHrCe2RKD"
      },
      "source": [
        "from spellchecker import SpellChecker"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww16wIq8Fzgg"
      },
      "source": [
        "Spell = SpellChecker()"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXWDTHjMORyY"
      },
      "source": [
        "doc = ['misspel', 'calandar', 'naccessary', 'dignity', 'bussiness', 'recive']"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIPc8ub6PQCJ",
        "outputId": "87dc7ca9-b84a-4adf-d76c-3d0f3fd57c01"
      },
      "source": [
        "for word in doc:\r\n",
        "  print(f\"{word} : {Spell.correction(word)}\")"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "misspel : misspelt\n",
            "calandar : calendar\n",
            "naccessary : necessary\n",
            "dignity : dignity\n",
            "bussiness : business\n",
            "recive : receive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7uZb3EEQKE4",
        "outputId": "c6383a16-fd48-48fb-c030-f803f352d7b4"
      },
      "source": [
        "for word in doc:\r\n",
        "  print(f\"{word} : {Spell.candidates(word)}\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "misspel : {'misspelt'}\n",
            "calandar : {'calendar'}\n",
            "naccessary : {'accessory', 'necessary'}\n",
            "dignity : {'dignity'}\n",
            "bussiness : {'fussiness', 'bossiness', 'business'}\n",
            "recive : {'recife', 'recipe', 'relive', 'revive', 'receive', 'recite'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flZhIlkpQz5n",
        "outputId": "9b348b59-3152-4b23-e246-7262e05d1c7e"
      },
      "source": [
        "pip install autocorrect"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: autocorrect in /usr/local/lib/python3.6/dist-packages (2.2.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfXktP4gRSMJ"
      },
      "source": [
        "from autocorrect import spell"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNneSMfVQ7hM",
        "outputId": "26364bc3-a6a4-4fc0-87db-32827393ed2a"
      },
      "source": [
        "for word in doc:\r\n",
        "  print(spell(word))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
            "missed\n",
            "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
            "calendar\n",
            "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
            "necessary\n",
            "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
            "dignity\n",
            "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
            "business\n",
            "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
            "receive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDp7YeUnRInG",
        "outputId": "03465b6f-791a-4e21-b12c-d99c026933be"
      },
      "source": [
        "pip install textblob"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.6/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crBC1Wv2Scbq"
      },
      "source": [
        "from textblob import TextBlob, Word"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV1rflVUSnjA"
      },
      "source": [
        "txt = TextBlob(\"He is very much accomadate in his new locotion\")"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHopj58HS5Nd",
        "outputId": "025e34df-8357-484f-ad93-8701bc62e266"
      },
      "source": [
        "for word in txt.words:\r\n",
        "  print(word,\":\",word.correct())"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "He : He\n",
            "is : is\n",
            "very : very\n",
            "much : much\n",
            "accomadate : accommodate\n",
            "in : in\n",
            "his : his\n",
            "new : new\n",
            "locotion : location\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}